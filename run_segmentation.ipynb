{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iba1 4G8 Segmentation and Localisation Script\n",
    "Created: 18/11/2022\n",
    "Creator: Emma Davis\n",
    "\n",
    "#### Workflow\n",
    "1. Read in 4G8 and Iba1 channels from same area of LCM.\n",
    "2. Segment microglia for Iba1 channel image and get coords.\n",
    "3. Segment plaques for 4G8 channel image and get coords.\n",
    "\n",
    "#### Workflow A\n",
    "4. For each microglia take the middle coord and find circle areas\n",
    "    with radius 20um and 50um.\n",
    "5. Check if plaque area overlaps with either circle areas, define\n",
    "    microglia with overlap in 20um as plaque niche microglia,\n",
    "    define microglia with no overlap in 50um as plaque far microglia.\n",
    "\n",
    "#### Workflow B 1\n",
    "4. Dilate plaque area to expand out 20um from current plaque border,\n",
    "    then 50um from current plaque border.\n",
    "5. For each microglia, check if any of its area lies in the plaque area.\n",
    "    Microglia with >30% area overlapping 20um ring are plaque niche,\n",
    "    microglia with <30% area overlapping 50um ring are far\n",
    "    \n",
    "#### Workflow B 2\n",
    "4. Dilate plaque area to expand out 20um from current plaque border,\n",
    "    then 50um from current plaque border.\n",
    "5. For each microglia, check if any of its area lies in the plaque area.\n",
    "    Any area within 20um ring of plaques should be cut as plaque niche,\n",
    "    any area outside 50um ring of plaques should be cut as plaque far.\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "#### Changelog\n",
    "\n",
    "02-02-2023: Added DAPI segmentation to more successfully define microglia in\n",
    "    preparation for morphological analysis. DAPI seg is using stardist.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "# IMPORTS\n",
    "import cv2\n",
    "from datetime import date, datetime\n",
    "import json\n",
    "from IPython.display import Image, display\n",
    "from math import ceil, pi\n",
    "from matplotlib import pyplot as plt\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import ImageOps, Image\n",
    "from PIL.TiffTags import TAGS\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import xmltodict\n",
    "import time\n",
    "import stardist\n",
    "from stardist.models import StarDist2D\n",
    "from stardist.plot import render_label\n",
    "from csbdeep.utils import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET WORKING DIR\n",
    "working_dir = \"/home/my_project/\"\n",
    "os.chdir(working_dir)\n",
    "# IMPORT PRETRAINED MODEL FOR MICROGLIA SEGMENTATION\n",
    "model = keras.models.load_model('microglia_seg_model_0423.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO TO DIRECTORY HOLDING ALL IMAGES TO SEGMENT\n",
    "# TESTING IBA1 DAPI SEG\n",
    "lcm_dir = \"/training_data/orig/microglia/\"\n",
    "\n",
    "iba1_imgs = sorted([file for file in os.listdir(lcm_dir) if file.endswith('.tif')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~ ALL FUNCTIONS ~~~~~~~~~~\n",
    "\n",
    "# CLASS TO BATCH IMAGES INTO NUMPY ARRAYS\n",
    "class ArrayHelper(keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size)\n",
    "            x[j] = img\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "            y[j] = np.expand_dims(img, 2)\n",
    "\n",
    "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "            y[j] -= 1\n",
    "        return x, y\n",
    "\n",
    "# FUNCTION TO PERFORM GAMMA CORRECTION ON LCM IMAGES\n",
    "def gammaCorrection(src, gamma):\n",
    "    invGamma = 1 / gamma\n",
    "\n",
    "    table = [((i / 255) ** invGamma) * 255 for i in range(256)]\n",
    "    table = np.array(table, np.uint8)\n",
    "\n",
    "    return cv2.LUT(src, table)\n",
    "\n",
    "# CROP IMAGE INTO 1920x1920 NO OVERLAP\n",
    "def crop(img_loc, path, height, width, k, page, area):\n",
    "    im = Image.open(img_loc)\n",
    "    imgwidth, imgheight = im.size\n",
    "    img_lst = []\n",
    "    for i in range(0,imgheight,height):\n",
    "        for j in range(0,imgwidth,width):\n",
    "            box = (j, i, j+width, i+height)\n",
    "            a = im.crop(box)\n",
    "            #display(a)\n",
    "            img_lst.append(a)\n",
    "            k +=1\n",
    "    return img_lst\n",
    "\n",
    "# CROP IMAGE INTO 1920x1920 WITH HALF OVERLAP\n",
    "def crop_ovrlp(img_loc, height, width, k, page, area):\n",
    "    im = Image.open(img_loc)\n",
    "    imgwidth, imgheight = im.size\n",
    "    img_lst = []\n",
    "    coord_lst = {}\n",
    "    counter = 0\n",
    "    for i in range(0,imgheight,ceil(height/2)):\n",
    "        for j in range(0,imgwidth,ceil(width/2)):\n",
    "            box = (j, i, j+width, i+height)\n",
    "            coord_lst[counter] = box\n",
    "            counter += 1\n",
    "            a = im.crop(box)\n",
    "            #display(a)\n",
    "            img_lst.append(a)\n",
    "            k +=1\n",
    "    return img_lst, coord_lst\n",
    "\n",
    "# DISPLAY PREDICTED TRIMAP\n",
    "def display_mask(i):\n",
    "    mask = np.argmax(val_preds[i], axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
    "    display(img)\n",
    "\n",
    "# CONVERT PREDICTED MASK TO BINARY\n",
    "def mask_to_binary(i):\n",
    "    mask = np.argmax(i, axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
    "    img = np.asarray(img)\n",
    "    proc_img = img.copy()\n",
    "    if np.all(proc_img==0):\n",
    "        img = Image.fromarray(proc_img)\n",
    "    else:\n",
    "        proc_img[proc_img == 0] = 255\n",
    "        proc_img[proc_img == 127] = 0\n",
    "        img = Image.fromarray(proc_img)\n",
    "    \n",
    "    if np.all(proc_img==255):\n",
    "        proc_img[proc_img == 255] = 0\n",
    "        img = Image.fromarray(proc_img)\n",
    "    \n",
    "    #display(img)\n",
    "    return img\n",
    "\n",
    "# FUNCTION TO ALLOW CHUNKING ON 5 COORDS AT A TIME\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "# FUNCTION TO EXPORT CONTOURS AS COORDS\n",
    "def contour_to_coords(final_contours, img_name, dir_path, dir_name, pxl_to_lcm):\n",
    "    total_area = 0\n",
    "    \n",
    "    # OPEN TIF IMAGES AND GET METADATA\n",
    "    img = Image.open(dir_path + \"/Iba1/\" + img_name + \".tif\")\n",
    "    meta_dict = {TAGS[key] : img.tag[key] for key in img.tag_v2}\n",
    "\n",
    "    # PARSE METADATA AS DICT\n",
    "    meta_dict['ImageDescription'] = str(meta_dict['ImageDescription']).replace(\n",
    "        '\\\\x00', '').replace(\"b\\'\", '').replace(\"\\\\r\\\\n\\'\", '')\n",
    "    meta_dict = xmltodict.parse(meta_dict['ImageDescription'])\n",
    "    \n",
    "    # DRAW CONTOURS WITH AREA LARGER THAN x, ALSO ADD TO LIST OF THRESHOLDED CONTOURS\n",
    "    # FOR ALL COORDINATES, ADD X AND Y OFFSET FROM METADATA TO MATCH LCM\n",
    "    x_offset = int(meta_dict['RecordList']['PictureInfo']['Picture']['StagePosition'][0]['#text'])\n",
    "    y_offset = int(meta_dict['RecordList']['PictureInfo']['Picture']['StagePosition'][1]['#text'])\n",
    "    \n",
    "    #final_contours = final_contours.astype(float)\n",
    "    \n",
    "    # PIX TO LCM\n",
    "    if pxl_to_lcm == 1:\n",
    "        for coord in final_contours:\n",
    "            coord[0][0] = float(coord[0][0]) /5.703\n",
    "            coord[0][1] = float(coord[0][1]) /5.703\n",
    "            coord[0][0] = float(coord[0][0]) + x_offset - 360\n",
    "            coord[0][1] = float(coord[0][1]) + y_offset - 397.7\n",
    "    \n",
    "    # EXPORT COORDS FOR LCM, AVERAGE AREA FOR ONE CELL IS 5000-6000\n",
    "    date = datetime.today()\n",
    "    date = date.strftime(\"%d.%m.%Y\")\n",
    "    time = datetime.now()\n",
    "    time = time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "    header = str(\"PALMRobo Elements\\nVersion:\\tV 4.9.0.0\\nDate, Time:\\t\" + str(date) + \"\\t\" +\n",
    "                 str(time) + \"\\n\\nMICROMETER\\nElements :\\n\\nType\\tColor\\tThickness\\tNo\\tLaser function\" +\n",
    "                 \"\\tCutShot\\tArea\\tZ\\tWell\\tObjective\\tComment\\tCoordinates\\t\\n\")\n",
    "\n",
    "    for i in range(len(final_contours)):\n",
    "        number_roi = i\n",
    "        area_roi = cv2.contourArea(final_contours[i])\n",
    "        total_area += area_roi\n",
    "        roi_str = str(\"\\nFreehand\\tgreen\\t2\\t\" + str(number_roi) + \"\\tCut\\t0,0\\t\" + str(area_roi) +\n",
    "                  \"\\t6910\\tmanual\\t40x - LD Plan-Neofluar 40x/0.6 Korr M27\\t.\\t\")\n",
    "        header = header + roi_str\n",
    "\n",
    "        # 5 COORDS MAX ON EACH LINE\n",
    "        for j in chunker(range(len(final_contours[i])), 5):\n",
    "            coords = final_contours[i]\n",
    "            line = \".\"\n",
    "            for k in j:\n",
    "                coord = coords[k]\n",
    "                line = line + \"\\t\" + str(coord[0][0]) + \",\" + str(coord[0][1])\n",
    "            header = header + \"\\n\" + line\n",
    "\n",
    "    # WRITE ELEMENTS\n",
    "    elems = open(dir_path + dir_name + img_name + \"_elems.txt\", \"w\")\n",
    "    n = elems.write(header)\n",
    "    elems.close()\n",
    "\n",
    "    print(\"Total area of ROIs: \" + str(total_area))\n",
    "    \n",
    "    return total_area\n",
    "\n",
    "# CONVERT PIXEL CONTOURS TO LCM CONTOUR COORDINATES\n",
    "def pxl_to_lcm(contour, x_offset, y_offset):\n",
    "    contour_lcm = contour.copy()\n",
    "    for coord in contour_lcm:\n",
    "        coord[0][0] = float(coord[0][0]) /5.703\n",
    "        coord[0][1] = float(coord[0][1]) /5.703\n",
    "        coord[0][0] = float(coord[0][0]) + x_offset - 360\n",
    "        coord[0][1] = float(coord[0][1]) + y_offset - 397.7\n",
    "    \n",
    "    return contour_lcm\n",
    "\n",
    "# MICROGLIA SEGMENTATION\n",
    "def microglia_segmentation(img_name, dir_path, denoise=2, gamma=0.9, dilate=15, erode=3):\n",
    "    # OPEN TIF IMAGES AND GET METADATA\n",
    "    img = Image.open(dir_path + img_name + \".tif\")\n",
    "    meta_dict = {TAGS[key] : img.tag[key] for key in img.tag_v2}\n",
    "\n",
    "    # PARSE METADATA AS DICT\n",
    "    meta_dict['ImageDescription'] = str(meta_dict['ImageDescription']).replace(\n",
    "        '\\\\x00', '').replace(\"b\\'\", '').replace(\"\\\\r\\\\n\\'\", '')\n",
    "    meta_dict = xmltodict.parse(meta_dict['ImageDescription'])\n",
    "\n",
    "    # DENOISE IMAGE\n",
    "    img = np.array(img)\n",
    "    img = cv2.fastNlMeansDenoising(img, h=denoise)#h=16\n",
    "\n",
    "    # ADJUST EXPOSURE/GAMMA\n",
    "    img = gammaCorrection(img, gamma=gamma)\n",
    "\n",
    "    # ADJUST CONTRAST VIA THRESHOLDING\n",
    "    #ret,img = cv2.threshold(img, 30, 255, cv2.THRESH_TOZERO)\n",
    "\n",
    "    # INCREASE CONTRAST\n",
    "    # converting to LAB color space\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l_channel, a, b = cv2.split(lab)\n",
    "\n",
    "    # Applying CLAHE to L-channel\n",
    "    # feel free to try different values for the limit and grid size:\n",
    "    clahe = cv2.createCLAHE(clipLimit=1, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l_channel)\n",
    "\n",
    "    # merge the CLAHE enhanced L-channel with the a and b channel\n",
    "    limg = cv2.merge((cl,a,b))\n",
    "\n",
    "    # Converting image from LAB Color model to BGR color spcae\n",
    "    img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    # DISPLAY IMPROVED IMAGE\n",
    "    plt.imshow(img, interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "    # SAVE CORRECTED IMAGE\n",
    "    img = Image.fromarray(img)\n",
    "\n",
    "    # CROP OUT BAR AT BOTTOM\n",
    "    img = img.crop((0, 0, 1936, 1400))\n",
    "    \n",
    "    # CHECK IF WHOLE CLEANED DIRECTORY EXISTS, IF NOT MAKE ONE AND SAVE IMG\n",
    "    if not os.path.exists(dir_path + \"/Iba1_whole_clean\"):\n",
    "        os.makedirs(dir_path + \"/Iba1_whole_clean\")\n",
    "\n",
    "    img.save(dir_path + \"/Iba1_whole_clean/\" + img_name + \".png\")\n",
    "\n",
    "    # CROP WHOLE IMAGE IN SLIDING WINDOW FASHION\n",
    "    img_lst, coord_lst = crop_ovrlp(dir_path + \"/Iba1_whole_clean/\" + img_name + \".png\",\n",
    "                                    384, 384, 0, 1, 417316)\n",
    "    \n",
    "    # CHECK IF WHOLE CLEANED DIRECTORY EXISTS, IF NOT MAKE ONE AND SAVE IMG\n",
    "    if not os.path.exists(dir_path + \"/Iba1_crops/\" + img_name):\n",
    "        os.makedirs(dir_path + \"/Iba1_crops/\" + img_name)\n",
    "    \n",
    "    # SAVE CROPPED IMAGES\n",
    "    for i in range(len(img_lst)):\n",
    "        img_lst[i].save(dir_path + \"/Iba1_crops/\" + img_name + \"/\" + str(i).zfill(3) + '.png')\n",
    "\n",
    "    # SAVE CROPPED COORDS AS TXT FILE\n",
    "    with open(dir_path + \"/Iba1_crops/\" + img_name + \"/coords.txt\", 'w') as file:\n",
    "        file.write(json.dumps(coord_lst))\n",
    "        \n",
    "    # SPLIT IMAGE INTO MANAGABLE CHUNKS FOR MODEL WHILE RETAINING COORDINATE SYSTEM IN AN ARRAY SOMEWHERE\n",
    "    input_dir = dir_path + \"/Iba1_crops/\" + img_name + \"/\"\n",
    "    target_dir = \"/training_data/augmented/masks/\"\n",
    "\n",
    "    img_size = (384, 384)\n",
    "    num_classes = 3\n",
    "    batch_size = 88\n",
    "\n",
    "    input_img_paths = sorted(\n",
    "        [\n",
    "            os.path.join(input_dir, fname)\n",
    "            for fname in os.listdir(input_dir)\n",
    "            if fname.endswith(\".png\")\n",
    "        ]\n",
    "    )\n",
    "    target_img_paths = sorted(\n",
    "        [\n",
    "            os.path.join(target_dir, fname)\n",
    "            for fname in os.listdir(target_dir)\n",
    "            if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"Number of samples:\", len(input_img_paths))\n",
    "\n",
    "    for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
    "        print(input_path, \"|\", target_path)\n",
    "\n",
    "    pred_gen_parallel = []\n",
    "    for x in range(int(batch_size/8)):\n",
    "        pred_gen_parallel.append(ArrayHelper(8, img_size, input_img_paths[x*8:(x*8)+8],\n",
    "                                         target_img_paths[x*8:(x*8)+8]))\n",
    "\n",
    "    # TIME PREDICTION\n",
    "    start = time.time()\n",
    "\n",
    "    # RUN PREDICTION IN CHUNKED BATCHES FOR SPEED\n",
    "    val_preds = []\n",
    "    for pred_gen in pred_gen_parallel:\n",
    "        val_preds.extend(model.predict(pred_gen))\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Prediction took \" + str(end - start) + \" seconds.\")\n",
    "    \n",
    "    # CHECK IF WHOLE CLEANED DIRECTORY EXISTS, IF NOT MAKE ONE AND SAVE IMG\n",
    "    if not os.path.exists(dir_path + \"/Iba1_crops_pred/\" + img_name):\n",
    "        os.makedirs(dir_path + \"/Iba1_crops_pred/\" + img_name)\n",
    "        \n",
    "    # MAKE DIRECTORY FOR COORDS\n",
    "    if not os.path.exists(dir_path + \"/Iba1_coords/\"):\n",
    "        os.makedirs(dir_path + \"/Iba1_coords/\")\n",
    "\n",
    "    # GET PREDICTED MASKS FOR ALL CROPS AND CONVERT TO BINARY BEFORE SAVING\n",
    "    for i in range(batch_size):\n",
    "        bin_mask = mask_to_binary(val_preds[i])\n",
    "        bin_mask.save(dir_path + \"/Iba1_crops_pred/\" + img_name + \"/\" + str(i).zfill(3) + '.png')\n",
    "\n",
    "    # STITCH UP CROPPED PREDICTED MASKS TO GET WHOLE PREDICTED MASK FOR IMAGE\n",
    "    mask = np.zeros((1728, 2304))\n",
    "\n",
    "    # GET COORDS BACK\n",
    "    with open(dir_path + \"/Iba1_crops/\" + img_name + \"/coords.txt\") as f:\n",
    "        coord_dict = f.read()\n",
    "    \n",
    "    coord_dict = json.loads(coord_dict)\n",
    "\n",
    "    for sub_mask, bbox in zip(val_preds, coord_dict.values()):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        sub_mask = np.asarray(mask_to_binary(sub_mask))\n",
    "        mask[y1:y2, x1:x2] += sub_mask\n",
    "\n",
    "    mask = ((mask>=1)*255.0).astype(np.uint8)\n",
    "    \n",
    "    # DILATE AND ERODE MASK\n",
    "    kernel = np.ones((dilate, dilate), np.uint8)\n",
    "    mask = cv2.dilate(mask, kernel)\n",
    "\n",
    "    kernel = np.ones((erode, erode), np.uint8)\n",
    "    mask = cv2.erode(mask, kernel)\n",
    "\n",
    "    mask = Image.fromarray(mask)\n",
    "\n",
    "    # CROP TO SAME SIZE AS ORIGINAL\n",
    "    mask = mask.crop((0, 0, 1936, 1400))\n",
    "\n",
    "    # NEED TO ROTATE 180 TO MAKE COORDS MATCH LCM\n",
    "    mask = mask.transpose(Image.ROTATE_180)\n",
    "    \n",
    "    # GET NUMERIC ROI COORDINATES FROM PREDICTIVE MASKS (OPENCV FIND CONTOURS ON BINARY MASK?\n",
    "    np_mask = np.array(mask)\n",
    "    contours, hierarchy = cv2.findContours(np_mask,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    orig_img = np.array(\n",
    "        Image.open(dir_path + \"/Iba1_whole_clean/\" + img_name + '.png'))\n",
    "\n",
    "    # ROTATE ORIGINAL IMAGE TO MATCH LCM COORD SYSTEM\n",
    "    orig_img = Image.fromarray(orig_img)\n",
    "    orig_img = orig_img.transpose(Image.ROTATE_180)\n",
    "    orig_img = np.asarray(orig_img)\n",
    "\n",
    "    # DRAW CONTOURS WITH AREA LARGER THAN x, ALSO ADD TO LIST OF THRESHOLDED CONTOURS\n",
    "    # FOR ALL COORDINATES, ADD X AND Y OFFSET FROM METADATA TO MATCH LCM\n",
    "    x_offset = int(meta_dict['RecordList']['PictureInfo']['Picture']['StagePosition'][0]['#text'])\n",
    "    y_offset = int(meta_dict['RecordList']['PictureInfo']['Picture']['StagePosition'][1]['#text'])\n",
    "\n",
    "    final_contours_lcm = []\n",
    "    final_contours_pxl = []\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 10000:#> 3000:\n",
    "            # ADDED CHECK TO SEE IF MICROGLIA HAS DAPI\n",
    "            #dapi_check = microglia_dapi_count(dapi_dict[str(img_name + \".tif\")], contour)\n",
    "            dapi_check = True\n",
    "            if dapi_check:\n",
    "                cv2.drawContours(orig_img, contour, -1, (0, 255, 0), 3)\n",
    "                final_contours_pxl.append(contour.copy())\n",
    "\n",
    "                for coord in contour:\n",
    "                    coord[0][0] /= 5.703\n",
    "                    coord[0][1] /= 5.703\n",
    "                    coord[0][0] += x_offset - 360\n",
    "                    coord[0][1] += y_offset - 397.7\n",
    "                final_contours_lcm.append(contour)\n",
    "\n",
    "    plt.imshow(orig_img,cmap='gray')\n",
    "\n",
    "    # SAVE IMAGE WITH ROIS DRAWN\n",
    "    orig_img = Image.fromarray(orig_img)\n",
    "    orig_img.save(dir_path + \"/Iba1_whole_clean/\" + img_name + \"_predROIs.png\")\n",
    "    \n",
    "    area = contour_to_coords(final_contours_lcm, img_name, dir_path, \"/Iba1_coords/\", 0)\n",
    "    return area, final_contours_lcm, final_contours_pxl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN IBA1 SEGMENTATION\n",
    "iba_rois_lcm = {}\n",
    "iba_rois_pxl = {}\n",
    "for iba1 in iba1_imgs:\n",
    "    # GET CONTOURS AND ADD TO DICT\n",
    "    area, contours_lcm, contours_pxl = microglia_segmentation(iba1.replace(\".tif\", \"\"), lcm_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local emlaz_env",
   "language": "python3",
   "name": "rik_local_emlaz_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
